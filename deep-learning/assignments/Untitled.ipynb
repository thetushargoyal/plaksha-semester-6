{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ff54b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9ddccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "416b613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.b1 = np.zeros((1, self.hidden_size))\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.b2 = np.zeros((1, self.output_size))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Forward propagation\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = softmax(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        # Backpropagation\n",
    "        m = X.shape[0]\n",
    "        dz2 = self.a2 - y\n",
    "        self.dW2 = np.dot(self.a1.T, dz2) / m\n",
    "        self.db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        dz1 = np.dot(dz2, self.W2.T) * sigmoid_derivative(self.z1)\n",
    "        self.dW1 = np.dot(X.T, dz1) / m\n",
    "        self.db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "    def update_params(self, optimizer):\n",
    "        optimizer.update_params(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2938c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update_params(self, nn):\n",
    "        pass  # SGD doesn't need additional update step\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.VdW1 = 0\n",
    "        self.Vdb1 = 0\n",
    "        self.VdW2 = 0\n",
    "        self.Vdb2 = 0\n",
    "        \n",
    "    def update_params(self, nn):\n",
    "        self.VdW1 = self.momentum * self.VdW1 + self.lr * nn.dW1\n",
    "        self.Vdb1 = self.momentum * self.Vdb1 + self.lr * nn.db1\n",
    "        self.VdW2 = self.momentum * self.VdW2 + self.lr * nn.dW2\n",
    "        self.Vdb2 = self.momentum * self.Vdb2 + self.lr * nn.db2\n",
    "        \n",
    "        nn.W1 -= self.VdW1\n",
    "        nn.b1 -= self.Vdb1\n",
    "        nn.W2 -= self.VdW2\n",
    "        nn.b2 -= self.Vdb2\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0\n",
    "        self.mW1 = 0\n",
    "        self.vW1 = 0\n",
    "        self.mb1 = 0\n",
    "        self.vb1 = 0\n",
    "        self.mW2 = 0\n",
    "        self.vW2 = 0\n",
    "        self.mb2 = 0\n",
    "        self.vb2 = 0\n",
    "        \n",
    "    def update_params(self, nn):\n",
    "        self.t += 1\n",
    "        self.mW1 = self.beta1 * self.mW1 + (1 - self.beta1) * nn.dW1\n",
    "        self.vW1 = self.beta2 * self.vW1 + (1 - self.beta2) * (nn.dW1 ** 2)\n",
    "        mW1_hat = self.mW1 / (1 - self.beta1 ** self.t)\n",
    "        vW1_hat = self.vW1 / (1 - self.beta2 ** self.t)\n",
    "        nn.W1 -= self.lr * mW1_hat / (np.sqrt(vW1_hat) + self.epsilon)\n",
    "        \n",
    "        self.mb1 = self.beta1 * self.mb1 + (1 - self.beta1) * nn.db1\n",
    "        self.vb1 = self.beta2 * self.vb1 + (1 - self.beta2) * (nn.db1 ** 2)\n",
    "        mb1_hat = self.mb1 / (1 - self.beta1 ** self.t)\n",
    "        vb1_hat = self.vb1 / (1 - self.beta2 ** self.t)\n",
    "        nn.b1 -= self.lr * mb1_hat / (np.sqrt(vb1_hat) + self.epsilon)\n",
    "        \n",
    "        self.mW2 = self.beta1 * self.mW2 + (1 - self.beta1) * nn.dW2\n",
    "        self.vW2 = self.beta2 * self.vW2 + (1 - self.beta2) * (nn.dW2 ** 2)\n",
    "        mW2_hat = self.mW2 / (1 - self.beta1 ** self.t)\n",
    "        vW2_hat = self.vW2 / (1 - self.beta2 ** self.t)\n",
    "        nn.W2 -= self.lr * mW2_hat / (np.sqrt(vW2_hat) + self.epsilon)\n",
    "        \n",
    "        self.mb2 = self.beta1 * self.mb2 + (1 - self.beta1) * nn.db2\n",
    "        self.vb2 = self.beta2 * self.vb2 + (1 - self.beta2) * (nn.db2 ** 2)\n",
    "        mb2_hat = self.mb2 / (1 - self.beta1 ** self.t)\n",
    "        vb2_hat = self.vb2 / (1 - self.beta2 ** self.t)\n",
    "        nn.b2 -= self.lr * mb2_hat / (np.sqrt(vb2_hat) + self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "216e5669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 13us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 170s 6us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 12s 3us/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NeuralNetwork' object has no attribute 'dW1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     nn\u001b[38;5;241m.\u001b[39mbackward(X_batch, y_batch, lr)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Update parameters using chosen optimizer\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Validation loss\u001b[39;00m\n\u001b[1;32m     45\u001b[0m y_val_pred \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mforward(X_val)\n",
      "Cell \u001b[0;32mIn[4], line 46\u001b[0m, in \u001b[0;36mAdam.update_params\u001b[0;34m(self, nn)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m, nn):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmW1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta1 \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmW1 \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta1) \u001b[38;5;241m*\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdW1\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvW1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta2 \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvW1 \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta2) \u001b[38;5;241m*\u001b[39m (nn\u001b[38;5;241m.\u001b[39mdW1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     48\u001b[0m     mW1_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmW1 \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NeuralNetwork' object has no attribute 'dW1'"
     ]
    }
   ],
   "source": [
    "# Load Fashion-MNIST data\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 784) / 255.0\n",
    "X_test = X_test.reshape(-1, 784) / 255.0\n",
    "y_train = np.eye(10)[y_train]\n",
    "y_test = np.eye(10)[y_test]\n",
    "\n",
    "# Split data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)\n",
    "\n",
    "# Initialize neural network\n",
    "input_size = 784\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "lr = 0.01\n",
    "\n",
    "# Choose optimizer\n",
    "optimizer = Adam(lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # Mini-batch\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = nn.forward(X_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = -np.sum(y_batch * np.log(y_pred)) / batch_size\n",
    "        \n",
    "        # Backpropagation\n",
    "        nn.backward(X_batch, y_batch, lr)\n",
    "        \n",
    "        # Update parameters using chosen optimizer\n",
    "        optimizer.update_params(nn)\n",
    "        \n",
    "    # Validation loss\n",
    "    y_val_pred = nn.forward(X_val)\n",
    "    val_loss = -np.sum(y_val * np.log(y_val_pred)) / len(X_val)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred = np.argmax(nn.forward(X_test), axis=1)\n",
    "accuracy = np.mean(y_test_pred == np.argmax(y_test, axis=1))\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f82104d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
